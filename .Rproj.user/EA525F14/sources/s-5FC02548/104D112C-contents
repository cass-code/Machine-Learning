---
# IMPORTANT: Change settings here, but DO NOT change the spacing.
# Remove comments and add values where applicable.
# The descriptions below should be self-explanatory

title: "Exloring Machine Learning"
#subtitle: "This will appear as Right Header"

documentclass: "elsarticle"

# --------- Thesis title (Optional - set to FALSE by default).
# You can move the details below around as you please.
Thesis_FP: FALSE
# Entry1: "An unbelievable study with a title spanning multiple lines."
# Entry2: "\\textbf{Nico Katzke}" # textbf for bold
# Entry3: "A thesis submitted toward the degree of Doctor of Philosophy"
# Uni_Logo: Tex/Logo.png # Place a logo in the indicated location (from your root, e.g. defaults to ~/Tex/Logo.png) and uncomment this line. Leave uncommented for no image
# Logo_width: 0.3 # If using a logo - use this to set width (size) of image
# Entry4: "Under the supervision of: \\vfill Prof. Joe Smith and Dr. Frank Smith"
# Entry5: "Stellenbosch University"
# Entry6: April 2020
# Entry7:
# Entry8:

# --------- Front Page
# Comment: ----- Follow this pattern for up to 5 authors
AddTitle: TRUE # Use FALSE when submitting to peer reviewed platform. This will remove author names.
Author1: "Cassandra Pengelly"  # First Author - note the thanks message displayed as an italic footnote of first page.
#Ref1: "Stellenbosch University" # First Author's Affiliation
Email1: "20346212\\@sun.ac.za" # First Author's Email address

# Author2: "John Smith"
# Ref2: "Some other Institution, Cape Town, South Africa"
# Email2: "John\\@gmail.com"
# CommonAffiliation_12: TRUE # If Author 1 and 2 have a common affiliation. Works with _13, _23, etc.

# Author3: "John Doe"
# Email3: "Joe\\@gmail.com"

#CorrespAuthor_1: TRUE  # If corresponding author is author 3, e.g., use CorrespAuthor_3: TRUE

# Comment out below to remove both. JEL Codes only given if keywords also given.
#keywords: "Multivariate GARCH \\sep Kalman Filter \\sep Copula" # Use \\sep to separate
#JELCodes: "L250 \\sep L100"

# ----- Manage headers and footers:
#BottomLFooter: $Title$
#BottomCFooter:
#TopLHeader: \leftmark # Adds section name at topleft. Remove comment to add it.
BottomRFooter: "\\footnotesize Page \\thepage" # Add a '#' before this line to remove footer.
addtoprule: TRUE
addfootrule: TRUE               # Use if footers added. Add '#' to remove line.

# --------- page margins:
margin: 2.3 # Sides
bottom: 2 # bottom
top: 2.5 # Top
HardSet_layout: TRUE # Hard-set the spacing of words in your document. This will stop LaTeX squashing text to fit on pages, e.g.
# This is done by hard-setting the spacing dimensions. Set to FALSE if you want LaTeX to optimize this for your paper.

# --------- Line numbers
linenumbers: FALSE # Used when submitting to journal

# ---------- References settings:
# You can download cls format here: https://www.zotero.org/ - simply search for your institution. You can also edit and save cls formats here: https://editor.citationstyles.org/about/
# Hit download, store it in Tex/ folder, and change reference below - easy.
bibliography: Tex/ref.bib       # Do not edit: Keep this naming convention and location.
csl: Tex/harvard-stellenbosch-university.csl # referencing format used.
# By default, the bibliography only displays the cited references. If you want to change this, you can comment out one of the following:
#nocite: '@*' # Add all items in bibliography, whether cited or not
# nocite: |  # add specific references that aren't cited
#  @grinold2000
#  @Someoneelse2010

# ---------- General:
RemovePreprintSubmittedTo: TRUE  # Removes the 'preprint submitted to...' at bottom of titlepage
Journal: "Journal of Finance"   # Journal that the paper will be submitting to, if RemovePreprintSubmittedTo is set to TRUE.
toc: FALSE                       # Add a table of contents
numbersections: TRUE             # Should sections (and thus figures and tables) be numbered?
fontsize: 11pt                  # Set fontsize
linestretch: 1.2                # Set distance between lines.
link-citations: TRUE            # This creates dynamic links to the papers in reference list.

### Adding additional latex packages:
# header-includes:
#    - \usepackage{colortbl} # Add additional packages here.

output:
  pdf_document:
    keep_tex: TRUE
    template: Tex/TexDefault.txt
    fig_width: 3.5 # Adjust default figure sizes. This can also be done in the chunks of the text.
    fig_height: 3.5
# abstract: |
#   Abstract to be written here. The abstract should not be too long and should provide the reader with a good understanding what you are writing about. Academic papers are not like novels where you keep the reader in suspense. To be effective in getting others to read your paper, be as open and concise about your findings here as possible. Ideally, upon reading your abstract, the reader should feel he / she must read your paper in entirety.
---

<!-- Setting default preferences for chunk options and loading in libraries: -->

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE, message = FALSE, warning = FALSE, fig.width = 6, fig.height = 5, fig.pos="H", fig.pos = 'H')

if(!require("tidyverse")) install.packages("tidyverse") # This will install the pakages I used for the project if they are not already loaded on your computer
if(!require("caret")) install.packages("caret")
if(!require("ggplot2")) install.packages("ggplot2")
if(!require("e1071")) install.packages("e1071")
if(!require("haven")) install.packages("haven")
if(!require("glmnet")) install.packages("glmnet")
if(!require("rsample")) install.packages("rsample")

library(caret)
library(ggplot2)
library(tidyverse)
library(e1071)
library(haven)
library(glmnet)
library(rsample)
```

<!-- ############################## -->
<!-- # Writing starts here #
<!-- ############################## -->

# Introduction \label{Introduction}

Economists have long been interested in the discussion of what factors influence a person's income. In more recent times, machine learning techniques have become one tool 

Machine learning race is a topic that has ... One of the uses of machine learning is it can be used to classify data.

This paper is split into two main parts: part 1 \ref{ML} applies machine learning techniques to the NIDS dataset \ref{Data} and part 2 \ref{sequel} makes use of sequel to manipulate the NIDS dataset. The machine learning section first compares the effectiveness of linear regression and regularised regression on predicting people's incomes. Then 5 classification algorithms - Linear Discriminant Analysis, Classification and Regression Trees, k-Nearest Neighbors, Support Vector Machines with a linear kernel and Random Forest - are evaluated on their accuracy in predicting a person's race. 

# Data  \label{Data}
NIDS discussion


# Machine Learning \label{ML}

 <!-- Data import -->
```{r}
library(haven)
Adult <- read_dta("data/nids-w5-v1.0.0-stata14/Adult_W5_Anon_V1.0.0.dta") # read in the NIDS data and name the data set Adult (which is the unit of observation)
#View(Adult) # Having a look at the data
#ls(Adult) 
```

##  Predicting Income \label{income}

Econometrics often makes use of regression analysis to model economic phenomena, test economic hypotheses and to forecast economic activity @econ [p.2]. A popular method in econometric regression modeling is that of Ordinary Least Squares; however, advances in machine learning have presented alternative/augmenting methods that may be (more) useful. One such augmenting method is K-fold cross validation, which evaluates the skill of machine learning models. As @kfold [p. 569] explain, in K-fold cross validation, a data set is randomly split into $k$ number of groups, of similar sizes. The first group is considered a validation set and the method is fitted to the other $k-1$ groups. 

Below, \ref{Regression} displays 3 different linear regressions of log of income using K-fold cross validation. For these regressions I built a function "linreg", which takes in a data frame, cleans and splits the data into a training (70% of the full data set) and a test set (30% of the full data set) and runs 3 different linear regressions, applying K-fold cross validation. The results of the regressions are then collected and stored in a list, which is returned by the function. I used k = 10, because empirically k=10 has been shown have test error rate estimates that have relatively low bias and variance (@k). I have also set seed in the function for reproducibility. 

Based on the Mincerian wage equation, Regression 1 \ref{Regression} regresses log of income on age, years of schooling and a dummy variable for if a person has a tertiary qualification or not. Regression 2 includes a variable for age-squared and the categorical variable race. Regression 3 includes a variable each for gender and marriage. The signs of the coefficients of the three regressions look fairly standard^[This at least is a good indication that the data is fairly well cleaned and is usable for testing the machine learning techniques] and most of the coefficients are statistically significant at 1% and lower.

```{r, results = 'asis'}
if(!require(huxtable)) install.packages(huxtable)
library(huxtable)
library(ggplot2)

Title <- "Log-Income Regression Output"
Label <- "Regression"
source("code/linreg.R")
#str(lm1)
models<- linreg(Adult)
lm1 <- models[[1]] 
lm2 <- models[[2]]
lm3 <- models[[3]]
htab <-
huxreg("Reg 1" = lm1, "Reg 2" = lm2, "Reg 3" = lm3, 
       statistics = character(0),
                note = "%stars%.") %>%
  set_caption(Title) %>%
  set_label(Label)
# More settings:
font_size(htab) <- 12
#call the table:
htab

```

Typically, economists are interested in evaluating the performance of a model, which can be done by assessing how well the model predicts the outcome variable. A useful statistical metric for measuring the performance of a regression model is the Root Mean Squared Error (RMSE) (@rmse).The RMSE measures the average error performed by the model in predicting the outcome for an observation. The mathematical formula for the RMSE is given by $$RMSE = \sqrt{(observeds - predicteds)^2/N)}$$ 
This implies lower the RMSE, the better the model performs.

\ref{Stats} reports the RMSES for both the training data and the test data. We can see that the RMSEs decreased from regression 1 to regression 3 for both the training and test data. This indicates that regression 3 is a better model than both regressions 1 and 2 (and that regression 2 has better predictive power than regression 1). The RMSEs

```{r, results = 'asis'}
# observations and RMSEs for each model
library(xtable)
source("code/linreg.R")
#str(lm1)
models<- linreg(Adult)
source("code/linreg_pred.R")
data1 <- linreg_pred(Adult) %>% tibble::as_tibble() 
r<- models[[4]]
#n<- models[[5]]
names<- data.frame("Regression" = c("Reg 1", "Reg 2", "Reg 3"))

data <- bind_cols(names, r, data1) %>% rename("RMSE Train" = x, "RMSE Test" = Reg) %>% tibble::as_tibble() 
table <- xtable(data, caption = "Regression RMSEs and Observations", label = "Stats")
print.xtable(table,
             # tabular.environment = "longtable",
             floating = TRUE,
             table.placement = 'H',
             include.rownames = FALSE,
             # scalebox = 0.3,
             comment = FALSE,
             caption.placement = 'top'
)


```


```{r, warning =  FALSE, fig.align = 'center', fig.height = 3, fig.width = 6, dev = 'png'}
list.files('code/', full.names = T, recursive = T) %>% as.list() %>% walk(~source(.))
r <- plot_ridge(Adult)
l <- plot_lasso(Adult)
```


```{r, echo = FALSE, include= TRUE, warning =  FALSE, results='asis'}
# list.files('code/', full.names = T, recursive = T) %>% as.list() %>% walk(~source(.))
# l <- plot_lasso(Adult)
```

```{r, echo = FALSE, include= TRUE, message=FALSE, warning =  FALSE, results = 'hide'}
df <- comp_RMSE(Adult)
df$variable <- as.character(df$variable)
df <- df %>% mutate(variable = replace(variable, variable == "train.RMSE", "RMSE Train")) %>%
  mutate(variable = replace(variable, variable == "test.RMSE", "RMSE Test"))

source("code/linreg.R")
#str(lm1)
models<- linreg(Adult)
source("code/linreg_pred.R")
data1 <- linreg_pred(Adult) %>% tibble::as_tibble() 
r<- models[[4]]
#n<- models[[5]]
names<- data.frame("name" = c("Reg 1", "Reg 2", "Reg 3"))

data <- bind_cols(names, r, data1) %>% rename("RMSE Train" = x, "RMSE Test" = Reg) %>% tibble::as_tibble()
dff <- data %>% tidyr::gather(variable, value, -name)
final <- bind_rows(df, dff)


  library(dplyr)
  library(gridExtra)
  library(grid)
  library(ggplot2)
  library(lattice)

  g <- ggplot(data=final, aes(x=name, y=value, fill=variable)) +
    geom_bar(stat="identity", position=position_dodge()) +
    scale_fill_hue(name="") +
    xlab("") + ylab("RMSE") +
    theme(text = element_text(size=12),
          axis.text.x = element_text(angle = 45, hjust = 1))
 
  plot <- grid.draw(grid.arrange(g, ncol=1, top="RMSE comparison"))
  
```

```{r, echo = FALSE, include= TRUE, warning =  FALSE, results = 'asis'}
df <- comp_RMSE(Adult)
df$variable <- as.character(df$variable)
df <- df %>% mutate(variable = replace(variable, variable == "train.RMSE", "RMSE Train")) %>%
  mutate(variable = replace(variable, variable == "test.RMSE", "RMSE Test"))

source("code/linreg.R")
#str(lm1)
models<- linreg(Adult)
source("code/linreg_pred.R")
data1 <- linreg_pred(Adult) %>% tibble::as_tibble() 
r<- models[[4]]
#n<- models[[5]]
names<- data.frame("name" = c("Reg 1", "Reg 2", "Reg 3"))

data <- bind_cols(names, r, data1) %>% rename("RMSE Train" = x, "RMSE Test" = Reg) %>% tibble::as_tibble()
dff <- data %>% tidyr::gather(variable, value, -name)
final <- bind_rows(df, dff)

library(dplyr)

df1 <- comp_table(Adult)
data <- final %>% tibble::as_tibble() %>% tidyr::spread(key = name,value = value) %>% as.data.frame() %>% select(-variable)
names<- data.frame("Dataset" = c("Test", "Train"))
data1 <- bind_cols(names, data)
#str(data)
library(xtable)

table <- xtable(data1, caption = "Model RMSEs", label = "RMSE")
print.xtable(table,
             # tabular.environment = "longtable",
             floating = TRUE,
             table.placement = 'H',
             include.rownames = FALSE,
             # scalebox = 0.3,
             comment = FALSE,
             caption.placement = 'top'
)

```


## Predicting Race \label{race}

```{r Figure2, warning =  FALSE, fig.align = 'center', fig.cap = "ML on \\label{Figure2}", fig.height = 3, fig.width = 6, dev = 'png'}
# list.files('code/', full.names = T, recursive = T) %>% as.list() %>% walk(~source(.))
source("code/clean.R")
cleaned <- clean(Adult)
source("code/ML_Plot.R")
unbal <-ML_Plot(cleaned) # running the machine learning algorithms on unbalanced data
plot(unbal)

### This balancing section was in a function but it kept breaking the document when I tried to knit so I unfortunately had to pull it out and put it here

library(ROSE)
library(tidyverse)
library(dplyr)
africa <- cleaned %>% filter(race == "African" | race == "White") # First splitting the dataset
coloured <- cleaned %>% filter(race == "Coloured" | race == "White")
  # balanced data set with under-sampling
  balanced.under <- ovun.sample(race~., data=africa,
                                p=0.5, seed=1,
                                method="under")$data

  # Balancing white and coloured
  balanced.under1 <- ovun.sample(race~., data=coloured,
                                 p=0.5, seed=1,
                                 method="under")$data

  colour <- balanced.under1 %>% filter(race == "Coloured")
  asia <- cleaned %>% filter(race == "Asian/Indian")

  bal_under <- bind_rows(balanced.under, colour, asia)
  bal1 <- bal_under %>% tibble::as_tibble()
# #str(cleaned)
# #str(bal1)
# source("code/balance_both.R")
# bal2 <- balance_both(Adult) %>% tibble::as_tibble()
source("code/ML_Plot.R")
balu <- ML_Plot(bal1) # running the machine learning algorithms on balanced (undersampled) data
plot(balu)
# source("code/ML_Plot.R")
# balb <- ML_Plot(bal2) # running the machine learning algorithms on balanced(both sampled) data
# plot(balb)
```


```{r Figure3 , warning =  FALSE, fig.align = 'center', fig.cap = "ML on balanced data", fig.height = 3, fig.width = 6, dev = 'png'}
# list.files('code/', full.names = T, recursive = T) %>% as.list() %>% walk(~source(.))
source("code/clean.R")
cleaned <- clean(Adult)

### This balancing section was also in a function but it kept breaking the document when I tried to knit so I unfortunately had to pull it out and put it here

library(ROSE)
library(tidyverse)
library(dplyr)

  africa <- cleaned %>% filter(race == "African" | race == "White") # First splitting the dataset
  coloured <- cleaned %>% filter(race == "Coloured" | race == "African")
  asian <- cleaned %>% filter(race == "Asian/Indian" | race == "African")

# balanced data set with both-sampling
  balanced.both <- ovun.sample(race~., data=africa,
                               p=0.5, seed=1,
                               method="both")$data

  white <- balanced.both %>% filter(race == "White")

  # Balancing African and coloured
  balanced.both1 <- ovun.sample(race~., data=coloured,
                                p=0.5, seed=1,
                                method="both")$data



  colour <- balanced.both1 %>% filter(race == "Coloured")

  # Balancing African and Asian
  balanced.both2 <- ovun.sample(race~., data=asian,
                                p=0.5, seed=1,
                                method="both")$data

  asia <- balanced.both2 %>% filter(race == "Asian/Indian")

  bal_both <- bind_rows(balanced.both1, white, asia)

  bal2 <-  bal_both %>% tibble::as_tibble()
  source("code/ML_Plot.R")
  balb <- ML_Plot(bal1) # running the machine learning algorithms on balanced(both sampled) data
  plot(balb)
```


![LDA Confusion Matrix]("images/Confusion Matrix LDA.png")
![LDA Statistics]("images/statslda.png")

```{r Figure4, warning =  FALSE, fig.align = 'center', fig.cap = "Confusion Matrices \\label{Figure3}", fig.height = 4, fig.width = 9, dev = 'png'}
list.files('code/', full.names = T, recursive = T) %>% as.list() %>% walk(~source(.))
cleaned <- clean(Adult)
predicted <- ML_Predict(cleaned)
g1 <-draw_confusion_matrix(predicted[[4]])
```


```{r Figure5, warning =  FALSE, fig.align = 'center', fig.cap = "Confusion Matrices \\label{Figure3}", fig.height = 4, fig.width = 6, dev = 'png'}

list.files('code/', full.names = T, recursive = T) %>% as.list() %>% walk(~source(.))
cleaned <- clean(Adult)
predicted <- ML_Predict(cleaned)

g2 <- draw_confusion_stats(predicted[[4]])
```

# Sequel \label{sequel}


# Conclusion

 @Texevier 


\newpage

# References {-}

<div id="refs"></div>


# Appendix {-}

## Appendix A {-}

Some appendix information here

## Appendix B {-}

