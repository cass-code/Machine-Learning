---
# IMPORTANT: Change settings here, but DO NOT change the spacing.
# Remove comments and add values where applicable.
# The descriptions below should be self-explanatory

#title: "Exploring Machine Learning: Predicting Income and Race"
#subtitle: "This will appear as Right Header"

documentclass: "elsarticle"

# --------- Thesis title (Optional - set to FALSE by default).
# You can move the details below around as you please.
Thesis_FP: TRUE
Entry1: "Exploring Machine Learning: Predicting Income and Race"
Entry2: "Cassandra Pengelly | 20346212" # textbf for bold
Entry3: "Data Science 871: Machine Learning Project"
Uni_Logo: images/m.jpg # Place a logo in the indicated location (from your root, e.g. defaults to ~/Tex/Logo.png) and uncomment this line. Leave uncommented for no image
Logo_width: 0.5
# Entry4: "Under the supervision of: \\vfill Prof. Joe Smith and Dr. Frank Smith"
# Entry5: "Stellenbosch University"
# Entry6: April 2020
# Entry7:
# Entry8:

# --------- Front Page
# Comment: ----- Follow this pattern for up to 5 authors
#AddTitle: TRUE # Use FALSE when submitting to peer reviewed platform. This will remove author names.
#Author1: "Cassandra Pengelly"  # First Author - note the thanks message displayed as an italic footnote of first page.
#Ref1: "Stellenbosch University" # First Author's Affiliation
#Email1: "20346212\\@sun.ac.za" # First Author's Email address

# Author2: "John Smith"
# Ref2: "Some other Institution, Cape Town, South Africa"
# Email2: "John\\@gmail.com"
# CommonAffiliation_12: TRUE # If Author 1 and 2 have a common affiliation. Works with _13, _23, etc.

# Author3: "John Doe"
# Email3: "Joe\\@gmail.com"

#CorrespAuthor_1: TRUE  # If corresponding author is author 3, e.g., use CorrespAuthor_3: TRUE

# Comment out below to remove both. JEL Codes only given if keywords also given.
#keywords: "Multivariate GARCH \\sep Kalman Filter \\sep Copula" # Use \\sep to separate
#JELCodes: "L250 \\sep L100"

# ----- Manage headers and footers:
#BottomLFooter: $Title$
#BottomCFooter:
#TopLHeader: \leftmark # Adds section name at topleft. Remove comment to add it.
BottomRFooter: "\\footnotesize Page \\thepage" # Add a '#' before this line to remove footer.
addtoprule: TRUE
addfootrule: TRUE               # Use if footers added. Add '#' to remove line.

# --------- page margins:
margin: 2.3 # Sides
bottom: 2 # bottom
top: 2.5 # Top
HardSet_layout: TRUE # Hard-set the spacing of words in your document. This will stop LaTeX squashing text to fit on pages, e.g.
# This is done by hard-setting the spacing dimensions. Set to FALSE if you want LaTeX to optimize this for your paper.

# --------- Line numbers
linenumbers: FALSE # Used when submitting to journal

# ---------- References settings:
# You can download cls format here: https://www.zotero.org/ - simply search for your institution. You can also edit and save cls formats here: https://editor.citationstyles.org/about/
# Hit download, store it in Tex/ folder, and change reference below - easy.
bibliography: Tex/ref.bib       # Do not edit: Keep this naming convention and location.
csl: Tex/harvard-stellenbosch-university.csl # referencing format used.
# By default, the bibliography only displays the cited references. If you want to change this, you can comment out one of the following:
#nocite: '@*' # Add all items in bibliography, whether cited or not
# nocite: |  # add specific references that aren't cited
#  @grinold2000
#  @Someoneelse2010

# ---------- General:
RemovePreprintSubmittedTo: TRUE  # Removes the 'preprint submitted to...' at bottom of titlepage
Journal: "Journal of Finance"   # Journal that the paper will be submitting to, if RemovePreprintSubmittedTo is set to TRUE.
toc: TRUE                       # Add a table of contents
numbersections: TRUE             # Should sections (and thus figures and tables) be numbered?
fontsize: 11pt                  # Set fontsize
linestretch: 1.2                # Set distance between lines.
link-citations: TRUE            # This creates dynamic links to the papers in reference list.

### Adding additional latex packages:
# header-includes:
#    - \usepackage{colortbl} # Add additional packages here.

output:
  pdf_document:
    keep_tex: TRUE
    template: Tex/TexDefault.txt
    fig_width: 3.5 # Adjust default figure sizes. This can also be done in the chunks of the text.
    fig_height: 3.5
# abstract: |
#   Abstract to be written here. The abstract should not be too long and should provide the reader with a good understanding what you are writing about. Academic papers are not like novels where you keep the reader in suspense. To be effective in getting others to read your paper, be as open and concise about your findings here as possible. Ideally, upon reading your abstract, the reader should feel he / she must read your paper in entirety.
---

<!-- Setting default preferences for chunk options and loading in libraries: -->

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE, message = FALSE, warning = FALSE, fig.width = 6, fig.height = 5, fig.pos="H", fig.pos = 'H')

if(!require("tidyverse")) install.packages("tidyverse") # This will install the pakages I used for the project if they are not already loaded on your computer
if(!require("caret")) install.packages("caret")
if(!require("ggplot2")) install.packages("ggplot2")
if(!require("e1071")) install.packages("e1071")
if(!require("haven")) install.packages("haven")
if(!require("glmnet")) install.packages("glmnet")
if(!require("rsample")) install.packages("rsample")

library(caret)
library(ggplot2)
library(tidyverse)
library(e1071)
library(haven)
library(glmnet)
library(rsample)
```

<!-- ############################## -->
<!-- # Writing starts here #
<!-- ############################## -->
\newpage

# Introduction \label{Introduction}

Machine learning is a field that develops algorithms designed to be applied to data sets, usually with the goal of prediction, classification, and clustering (@eco). Given that economists often work with data (mostly using econometrics and statistical modeling), machine learning could be a valuable addition to the economic toolkit. One of the advantages of machine learning algorithms is that they can handle large data sets that are multi-dimensional and multi-variety. With there being a significant increase in data availability for economics and other fields, machine learning offers researchers a method to process and exploit large volumes of data more efficiently than traditional statistical methods (@ecoml). Machine learning also offers economists the opportunity to build more flexible models, which is useful since economic theory is often vague about the specific functional forms of objects to be estimated. 

While linear regression analysis is commonly used in labour economics to decompose wages, it would be interesting to see if other machine learning algorithms could provide insight on such analysis. This paper therefore investigates how well machine learning techniques can predict income, given certain predictive variables. The second part of the paper explores how well race can be predicted using machine learning. The focus of this essay is on executing machine learning algorithms and demonstrating the use of SQL tools, rather than the economic interpretation of the model results. I include some discussion of the code used to generate the results as well as some of the intuition behind the machine learning methods.

This paper^[This assignment was written using the package by @Texevier] is structured as follows. First, the data set - NIDS - is discussed in section \ref{Data}. Then the methodology is explained in section \ref{Meth}, which focuses on the use of SQL. Section \ref{ML} applies machine learning techniques to the NIDS data set, and comprises two subsections. The first subsection (\ref{income}) compares the effectiveness of linear regression and regularized regression on predicting people's incomes. The second subsection (\ref{race}) evaluates 5 classification algorithms - Linear Discriminant Analysis, Classification and Regression Trees, k-Nearest Neighbors, Support Vector Machines with a linear kernel and Random Forest - on their accuracy in predicting a person's race. The final section (\ref{con}) concludes^[This project can be found on Github at https://github.com/cass-code/Machine-Learning.git]. 

# Data  \label{Data}
The data used for this assignment was sourced from the National Income Dynamics Survey (NIDS) (@nids). The survey is a nationally representative household panel study, which started in 2008 with a group of over 28,000 individuals from 7,300 households. The same households are surveyed every 2 years for NIDS. The latest survey - wave 5 - was conducted in 2017. For wave 5, a total of 39,434 individuals were interviewed; 20,113 of which were part of the original study - wave 1 - and 2,016 were from a top-up sample. NIDS is funded by the Department of Planning, Monitoring and Evaluation and the survey is implemented by the Southern Africa Labour and Development Research Unit (SALDRU) at the University of Cape Town. The data set is comprehensive and covers topics relating to poverty, health, household composition, mortality, expenditure, income and employment. The NIDS data set is partitioned into different units of observations (e.g. adults, children, household etc.); for the machine learning component of this assignment, data from wave 5 was used, with adults as the unit of observation. 

One weakness of the NIDS data set is that it suffers from the common problem that households at the higher end of the income distribution tend to be underrepresented. This could be explained by the fact that the rich refuse to fill out forms or they underreport their incomes. Because race and income are highly correlated in South Africa, this could also imply that white people are undersampled. 

# Methodology \label{Meth}
Before exploring the data, I first put it into a relational database. Relational databases are useful because they organize data into tables which can be linked based on data common to each. This allows for entirely new tables to be fetched/created from data in one or more tables with a single query. To communicate with the relational database, I used Structured Query Language (SQL) via SQLite ^[I found I struggle quite a bit using SQL but it has got easier with practice.]. To start investigating and visualising the data, I first ran the code in the R script called SQL. I opened a new connection and called it nids, and wrote the NIDS data, which included waves 2-5, into tables in the NIDS database. I used a few lines of code to check what tables were in nids and what their source was. Then I started to explore the NIDS wave 5 data, for example, looking at the column names. I queried the data and selected 7 variables for analysis: date of birth, income, gender, marital status, race, years of schooling and tertiary qualification. I used date of birth to calculate the variable 'age'. 

I cleaned the data by renaming the variables to be reader-friendly and removed values that were nonsensical (e.g. negative years of schooling) by applying filters to the data. I wanted to see the proportion of the races so I first manipulated the data in the SQL script and then used the function 'show_query' to get the code for SQL. I copied this into the r chunk in the markdown file and then graphed the results using ggplot2. The code below shows some of this process and output:

```{r, echo=TRUE}
library(DBI)
nids <- DBI::dbConnect(RSQLite::SQLite(), "data/nids-db~output.sqlite")
```

```{sql, connection=nids, output.var = "mydataframe", echo=TRUE}
SELECT `race`, COUNT(*) AS `n`
FROM (SELECT `w5_a_dob_y`, `w5_a_gen` AS `gender`, `w5_a_popgrp` AS `race`, 
`w5_a_em1pay` AS `income`, `w5_a_mar` AS `married`, `w5_a_edschgrd` AS `school`, 
`w5_a_edter` AS `tertiary` FROM `wave5`) WHERE (`race` > 0.0 AND `race` < 5.0)
GROUP BY `race`
```

```{r, echo=FALSE,message=FALSE, warning=FALSE, fig.cap = "Race Proportion\\label{rac}"}
library(ggplot2)
library(tidyverse)
library(kableExtra)

ggplot(mydataframe) +
    geom_col(aes(x = race, y = n,)) +
    xlab("Race") +
  ylab("People") +
  ggtitle("Race Proportions")

names<- data.frame("Race" = c("1 = African", "2 = Coloured", "3 = Asian/Indian", "4 = White")) %>% tibble::as_tibble() 
kable(names)
```

The bar graph above shows that the majority of people sampled are African and the smallest proportion are Asian/Indian. In general the proportions appear to match the race distribution of the South African population. The histogram below reports the number of years of schooling. We can see that the majority of the sample has had 12 years of schooling. The data for this graph was also manipulated using the SQL script and then the SQL code was copied into the r chunk. 

```{sql, connection=nids, output.var = "df", echo=FALSE, fig.cap = "Years of schooling\\label{schol}"} 
SELECT *
FROM (SELECT *
FROM (SELECT 2021.0 - `w5_a_dob_y` AS `age`, `w5_a_gen` AS `gender`, 
`w5_a_popgrp` AS `race`, `w5_a_em1pay` AS `income`, 
`w5_a_mar` AS `married`, `w5_a_edschgrd` AS `school`, 
`w5_a_edter` AS `tertiary`FROM `wave5`)
WHERE (`race` > 0.0 AND `race` < 5.0))
WHERE (`school` > 0.0 AND `school` < 16.0)
```

```{r, echo=FALSE, message=FALSE, warning=FALSE}
library(ggplot2)
library(tidyverse)

    ggplot(df, aes(x=school, fill = school)) + geom_histogram() +
    xlab("Years of Schooling") +
  ylab("People") +
  ggtitle("Schooling Distribution")
```

```{sql, connection=nids, output.var = "df1", echo=FALSE}
SELECT `married`, COUNT(*) AS `n`
FROM (SELECT 2021.0 - `w5_a_dob_y` AS `age`, `w5_a_gen` AS `gender`, `w5_a_popgrp` AS `race`, `w5_a_em1pay` AS `income`, `w5_a_mar` AS `married`, `w5_a_edschgrd` AS `school`, `w5_a_edter` AS `tertiary`
FROM `wave5`)
WHERE (`married` < 3.0 AND `married` > 0.0)
GROUP BY `married`
```

```{r, echo=FALSE, message=FALSE, warning=FALSE, fig.cap = "Married Proportion\\label{mar}"}
library(ggplot2)
library(tidyverse)
library(kableExtra)

ggplot(df1) +
    geom_col(aes(x = married, y = n,)) +
    xlab("Married") +
  ylab("People") +
  ggtitle("Married Proportions")

names<- data.frame("Married" = c("1 = Married", "2 = Not married")) %>% tibble::as_tibble() 
kable(names) 
```
Looking at the graph above, we see the proportion of people who are married versus those that are not; clearly there are more people married than unmarried in this sample.

If I had wanted to use more than one of the NIDS data sets, say wave 4 and 5, I could merge the two data sets using the SQL code below. I lightly cleaned both data sets beforehand and then used the union_all function to merge them.
```{sql, connection=nids, output.var = "df2", echo=TRUE}
SELECT 2021.0 - `w5_a_dob_y` AS `age`, `w5_a_gen` AS `gender`, 
`w5_a_popgrp` AS `race`, `w5_a_em1pay` AS `income`, 
`w5_a_mar` AS `married`, `w5_a_edschgrd` AS `school`, 
`w5_a_edter` AS `tertiary` FROM `wave5`
UNION ALL
SELECT 2021.0 - `w4_a_dob_y` AS `age`, `w4_a_gen` AS `gender`, 
`w4_a_popgrp` AS `race`, `w4_a_em1pay` AS `income`, 
`w4_a_mar` AS `married`, `w4_a_edschgrd` AS `school`, 
`w4_a_edter` AS `tertiary` FROM `wave4`
```
The table below provides a glimpse into the first 5 rows of the newly joined data set. I output a variable in the previous chunk as a dataframe and then displayed the dataframe in a table in the next r chunk. However, I could have used the function 'head' in the SQL script on the joined dataframe and the used the 'collect()' function to save the output to a global object and then displayed that object as a dataframe. 
```{r, echo=FALSE,message=FALSE, warning=FALSE}
library(kableExtra)
kable(head(df2, n=5))
```

After the initial data exploration I decided to focus on predicting income and race using machine learning techniques, which are presented in the following section. 

# Machine Learning \label{ML}

 <!-- Data import -->
```{r}
library(haven)
Adult <- read_dta("data/Adult_W5_Anon_V1.0.0.dta") # read in the NIDS data and name the data set Adult (which is the unit of observation)
#View(Adult) # Having a look at the data
#ls(Adult) 
```

##  Predicting Income \label{income}

Econometrics often makes use of regression analysis to model economic phenomena, test economic hypotheses and to forecast economic activity @econ [p.2]. A popular method in econometric regression modeling is that of Ordinary Least Squares; however, advances in machine learning have presented alternative/augmenting methods that may be (more) useful. One such augmenting method is K-fold cross validation, which evaluates the skill of machine learning models. As @kfold [p. 569] explain, in K-fold cross validation, a data set is randomly split into $k$ number of groups, of similar sizes. The first group is considered a validation set and the method is fitted to the other $k-1$ groups. 

Below, \ref{Regression} displays 3 different linear regressions of log of income using K-fold cross validation. For these regressions I built a function 'linreg', which takes in a data frame, cleans and splits the data into a training (70% of the full data set) and a test set (30% of the full data set) and runs 3 different linear regressions, applying K-fold cross validation. The sample size for the regressions amounts to 4258 observations, with the training and testing sets amounting to 2982 and 1276 respectively. The results of the regressions are then collected and stored in a list, which is returned by the function. I use k = 10, because empirically k=10 has been shown to have test error rate estimates that have relatively low bias and variance (@k). I have also set seed in the function for reproducibility. 

Based on the Mincerian wage equation, Regression 1 (see \ref{Regression}) regresses log of income on age, years of schooling and a dummy variable for if a person has a tertiary qualification or not. Regression 2 includes a variable for age-squared and the categorical variable race. Regression 3 includes a variable each for gender and marriage. The signs of the coefficients of the three regressions look fairly standard^[This at least is a good indication that the data is fairly well cleaned and is usable for testing the machine learning techniques] and most of the coefficients are statistically significant at 1% and lower. For the variables whose coefficients are not statistically significant, labour market literature and economic theory suggest that they are important controls and should be included, which justifies their presence.

```{r, results = 'asis'}
if(!require(huxtable)) install.packages(huxtable)
library(huxtable)
library(ggplot2)

Title <- "Log-Income Regression Output"
Label <- "Regression"
source("code/linreg.R")
models<- linreg(Adult)
lm1 <- models[[1]] 
lm2 <- models[[2]]
lm3 <- models[[3]]
htab <-
huxreg("Reg 1" = lm1, "Reg 2" = lm2, "Reg 3" = lm3, 
       statistics = character(0),
                note = "%stars%.") %>%
  set_caption(Title) %>%
  set_label(Label)
font_size(htab) <- 12
htab #call the table
```

Typically, economists are interested in evaluating the performance of a model, which can be done by assessing how well the model predicts the outcome variable. A useful statistical metric for measuring the performance of a regression model is the Root Mean Squared Error (RMSE) (@rmse).The RMSE measures the average error performed by the model in predicting the outcome for an observation. The mathematical formula for the RMSE is given by $$RMSE = mean(\sqrt{(observeds - predicteds)^2)})$$ 
This implies lower the RMSE, the better the model performs.

Table \ref{Stats} reports the RMSES for both the training data and the test data. We can see that the RMSEs decreased from regression 1 to regression 3 for both the training and test data. This indicates that regression 3 is a better model than both regressions 1 and 2 (and that regression 2 has better predictive power than regression 1). The RMSEs for the regression based on the training data are lower than for the test data. However, the RMSEs are close enough between the two data sets for all 3 models that the out-of-sample performance is fair; it does not seem that any of the models have been overfitted to the training data.

```{r, results = 'asis'}
# observations and RMSEs for each model
library(xtable)
library(kableExtra)
source("code/linreg.R")
#str(lm1)
models<- linreg(Adult)
source("code/linreg_pred.R")
data1 <- linreg_pred(Adult) %>% tibble::as_tibble() 
r<- models[[4]]
#n<- models[[5]]
names<- data.frame("Regression" = c("Reg 1", "Reg 2", "Reg 3"))

data <- bind_cols(names, r, data1) %>% rename("RMSE Train" = x, "RMSE Test" = Reg) %>% tibble::as_tibble() 
table <- xtable(data, caption = "Regression RMSEs and Observations", label = "Stats")
print.xtable(table,
             # tabular.environment = "longtable",
             floating = TRUE,
             table.placement = 'H',
             include.rownames = FALSE,
             # scalebox = 0.3,
             comment = FALSE,
             caption.placement = 'top'
)
```
The table above shows that as more explanatory variables are added to the regression, it appears that the model improves in predictive power. However, it could actually be the case that the model is over fitting the data. One method of addressing this issue is to use regularized regression, which constrains the estimated coefficients. It does this by introducing a penalty parameter in the objective function such that the sum of the sum of squared errors and the penalty parameter is minimised. Two common penalty parameters include the ridge and lasso methods. 

As @ridg explains: starting with a linear regression of the form $Y=X \beta+\varepsilon$ we can estimate the coefficients by ridge regression. This requires that \ref{eq1} is solved: 

\begin{align}
\hat{\beta}(\text{ridge})=\arg \min _{\beta \in \mathbb{R}^{p}} \frac{1}{n}\|Y-X \beta\|_{2}^{2}+\lambda\|\beta\|_{2}^{2} \label{eq1}
\end{align} 

with $\lambda>0$ as the regularization parameter (also known as the tuning, penalty, or complexity parameter). Ridge regression involves shrinking the coefficients of correlated explanatory variables equally towards zero.

The lasso estimator requires that the optimization problem (\ref{eq2}) below is solved:

\begin{align}
\hat{\beta}(\text{lasso})=\underset{{\beta}}{\operatorname{argmin}}\|\mathbf{Y}-\mathbf{X} {\beta}\|_{2}^{2}+\lambda\|\beta\|_{1} \label{eq2}
\end{align}
with $\|\beta\|_{1}=\sum_{j=1}^{p}\left|\beta_{j}\right|$ as the $\ell_{1}$ -norm penalty on $\beta$. This ensures the sparsity principle is adhered to in the solution. Again $\lambda$ is a tuning parameter. For a specific $\lambda$ value, the $\ell_{1}$ penalty allows the lasso to regularize the least squares fit and shrink some components of $\hat{\beta}$ to zero (@lass). The lasso model suffers when there is high degree of correlation among explanatory variables. The model will arbitrarily choose one predictor and ignore the others, and the model breaks down when all predictors are identical. The lasso tuning parameter expects many of the coefficients to be near zero, and only a small subset to be larger (@lass).

To apply the ridge and lasso methods I created two functions: 'plot_ridge' and 'plot_lasso'. These functions use the 'glmnet' package (@glm) to run the regressions and return a plot of the results, which are reported below. The tuning parameter here is given by $\lambda$. Initially, as $\lambda$ increases there is a decrease in the mean-squared error (MSE) for the lasso method, where the first dotted line indicates the lowest MSE. For the smaller values of lambda, this means that the lasso models has improved upon the OLS model. In the ridge model, as $\lambda$ increases, the coefficient sizes are being reduced but the number of coefficients remains the same. In the lasso model, after the log of $\lambda$ increases to more than -3, the number of parameters starts to decrease. 

```{r, warning =  FALSE, include=FALSE, fig.align = 'center', fig.height = 3, fig.width = 6, dev = 'png'}
## plot the ridge and lasso models MSEs and parameter values
## I saved these graphs as ridge.png and lasso.png

list.files('code/', full.names = T, recursive = T) %>% as.list() %>% walk(~source(.))
r <- plot_ridge(Adult)
l <- plot_lasso(Adult)
```

<center>

![Ridge Model]("images/ridge.png") 

</center>

<center>

![Lasso Model]("images/lasso.png")

</center>

```{r, echo = FALSE, include= FALSE, message=FALSE, warning =  FALSE, results = 'FALSE'}
## plot the bar graph for comparing all the RMSEs
## I saved the graph as RMSE.png
list.files('code/', full.names = T, recursive = T) %>% as.list() %>% walk(~source(.))
df <- comp_RMSE(Adult)
df$variable <- as.character(df$variable)
df <- df %>% mutate(variable = replace(variable, variable == "train.RMSE", "RMSE Train")) %>%
  mutate(variable = replace(variable, variable == "test.RMSE", "RMSE Test"))

source("code/linreg.R")
#str(lm1)
models<- linreg(Adult)
source("code/linreg_pred.R")
data1 <- linreg_pred(Adult) %>% tibble::as_tibble()
r<- models[[4]]
#n<- models[[5]]
names<- data.frame("name" = c("Reg 1", "Reg 2", "Reg 3"))

data <- bind_cols(names, r, data1) %>% rename("RMSE Train" = x, "RMSE Test" = Reg) %>% tibble::as_tibble()
dff <- data %>% tidyr::gather(variable, value, -name)
final <- bind_rows(df, dff)

  library(dplyr)
  library(gridExtra)
  library(grid)
  library(ggplot2)
  library(lattice)

  g <- ggplot(data=final, aes(x=name, y=value, fill=variable)) +
    geom_bar(stat="identity", position=position_dodge()) +
    scale_fill_hue(name="") +
    xlab("") + ylab("RMSE") +
    theme(text = element_text(size=12),
          axis.text.x = element_text(angle = 45, hjust = 1))

  plot <- grid.draw(grid.arrange(g, ncol=1, top="RMSE comparison"))
  
```

I then created a function 'comp_RMSE', which finds the $\lambda$ that minimises the RMSEs for the ridge and lasso models and records the minimised RMSEs. These models are then used to predict the log of income for the test data. The bar chart below displays the RMSEs for the ridge and lasso models and compares them to a base linear regression and the three regressions from \ref{Regression}. Here, I adjusted the code from @ridge tutorial.

The graph shows that the RMSEs are lower for the training data than for the test data for all the models, which could indicate the data is being overfitted. The lasso model performs marginally better than the ridge and linear model on the training and test sets. However, out of all the models regression 3 performs the best on the test set, while the lasso model performs the best on the training set. The disappointing performance of the ridge model could be explained by the fact that there are few predictor variables. Ridge regression performs well when there are many explanatory variables that each have a small effect on the outcome variable (@lass). We can see from the earlier regressions that education, specifically the presence of tertiary education, has a large effect on income, with the other predictors having a varying degree of influence.

<center>

![RMSE comparison]("images/RMSE.png")

</center>

While economists are interested in predicting wages based on variables such as race and education, it might be interesting to see if race can be predicted using income and other variables. The next section explores this idea by applying classification algorithms on the NIDS data set.

## Predicting Race \label{race}

This section investigates how well race can be predicted based on income, education, age, gender and marital status. 5 different machine learning classification methods were implemented: Linear Discriminant Analysis, Classification and Regression Trees, k-Nearest Neighbors, Support Vector Machines with a linear kernel and Random Forest. Each method is detailed below, followed by a discussion of the results of their application to the data. Each method was applied to 3 different data sets: first to an unbalanced data set, then to data set that was balanced using undersampling, and finally to a data set that was balanced using under and oversampling methods. 

Linear Discriminant Analysis (LDA), is a statistical a method that finds a linear combination of features that characterizes two or more categories of observations. This combination can then be used as a linear classifier. Or this combination can be used to reduce the number of variables and dimensions of a classification problem, without there being a significant information loss (@lda). LDA assumes that each variable follows a normal distribution. It also assumes that each variable has the same variance. To make predictions, LDA estimates the likelihood that a new set of features belongs to each group. The group that has the highest probability is the output group and the prediction is made. To estimate these probabilities, Bayes Theorem is used. In the figures (\ref{fig1}, \ref{fig2}, \ref{fig3}) below, the results for the LDA model are labeled as 'lda'.

The second method used to classify the data was Classification and Regression Trees (CART). As @cart [p.4] explains, CART analysis is a type of binary recursive partitioning, which involves the splitting of a node in a decision tree into two groups. Each node can be further split into two nodes (known as child nodes), and the original node is termed a parent node. This process can be applied several times over, hence the "recursive" part of binary recursive partitioning. Based on an exhaustive search of all possibilities, CART finds “splitting” variables. One of the advantages of using CART is it does not make assumptions about the underlying distribution of values of the explanatory variables.

There are essentially four steps in the CART process. The first step involves the tree building, where a tree is constructed via a recursive splitting of nodes. Each of the nodes is assigned a predicted group, based off of the distribution of groups in the training data, and the decision cost matrix. The second step involves stopping the process of tree building. The third step comprises tree "pruning", which involves reducing the number of nodes to produce a sequence of simpler trees. The final step is where the optimal tree is selected from the pruned trees based on which tree fits the information of the training data without overfitting it (@cart [p.6]). The results for the CART model are labeled as 'cart' in the figures (\ref{fig1}, \ref{fig2}, \ref{fig3}) below.

The third machine learning technique is known as k-Nearest Neighbors (KNN). As @book explaines, KNN predicts each observation based on how similar it is to other observations. KNN is a supervised/memory-based machine learning algorithm. As such, it has no closed form and when given new unlabeled data, KNN depends on labeled input data to learn a function that produces the relevant output. One of the disadvantages of using KNN is that it is sensitive to noisy predictor variables since these cause similar samples to have greater variability in distance values as well as larger magnitudes. The model will perform poorly if there are too many outliers in the sample. The results for the KNN model are labeled as 'knn' in the charts (\ref{fig1}, \ref{fig2}, \ref{fig3}) below.

Support Vector Machines with a linear kernel (SVM) is one of the more advanced algorithms that is used. According to @book, SVM aims to identify a hyperplane in an N-dimensional space that distinctly classifies the data points. There are many possible hyperplanes that could be chosen to partition two groups of data points. Identifying a hyperplane that distinctly classifies the data involves finding a plane that has the maximum margin. Data points that lie closer to the hyperplane and influence the orientation and position of the plane are known as support vectors. These vectors are used to maximize the margin of the classifier and help construct the SVM (@svm). The results for the SVM model are labeled as 'svm' in the figures (\ref{fig1}, \ref{fig2}, \ref{fig3}) below.

The final model used is Random Forests (RF), which is an algorithm that comprises many decorrelated decision trees (@book). RF generates a ‘forest’, which is trained by bootstrap or bagging aggregating. RF takes the average of the predictions of the trees in the forest and uses this mean to predict outcomes. The accuracy of the prediction can be improved by increasing the number of trees. One advantage of using RF is that it does not require extensive hyperparameter tuning (@book). In the plots below (\ref{fig1}, \ref{fig2}, \ref{fig3}), the RF results are labeled as 'rf'.

I built a function 'ML_plot' that applies the abovementioned algorithms - using 10 fold cross validation where necessary - and returns a plot of the accuracy and kappa results for each. For this, I adapted the code from a tutorial by @ml. Again a split of 70%/30% for the training and testing data was used. The first plot \ref{fig1} shows the results of the algorithms on unbalanced NIDS data. The second figure \ref{fig2} shows the results for the undersampled data and the last figure shows the outcomes for the undersampled and oversampled data \ref{fig3}. To balance^[I originally wrote a function that contained the code for balancing the data. Unfortunately, calling the function broke my document when knitting. To get around this I balanced the data in an r chunk in the document, and saved the plots as images.] the data (i.e. get a more equal distribution of the different races), I used a package called ROSE by @rose. I first undersampled the majority class, African; unfortunately, this method means the sample size is reduced, which could negatively impact the prediction accuracy. To compensate for this, I then also applied oversampling, which involves generating more observations from the minority classes by replicating the samples from the minority groups. The results in figure \ref{fig3} are based on a data set where there was some undersampling of African and some oversampling of the Coloured, Asian/Indian and White groups.

```{r unbalanced, warning =  FALSE, include= FALSE, fig.align = 'center', fig.cap = "ML on unbalanced data", fig.height = 3, fig.width = 6, dev = 'png'}
list.files('code/', full.names = T, recursive = T) %>% as.list() %>% walk(~source(.))
cleaned <- clean(Adult)
unbal <-ML_Plot(cleaned) # running the machine learning algorithms on unbalanced data
plot(unbal) # I saved this plot as an image called unbal.png
```

```{r balance1, include = FALSE, warning =  FALSE, fig.align = 'center', fig.cap = "ML on balanced data (undersampled) \\label{Figure2}", fig.height = 3, fig.width = 6, dev = 'png'}
### This balancing section was in a function but it kept breaking the document when I tried to knit so I unfortunately had to pull it out and put it here
## balancing the data using under sampling methods 
# library(ROSE)
# library(tidyverse)
# library(dplyr)
# africa <- cleaned %>% filter(race == "African" | race == "White") # First splitting the dataset
# coloured <- cleaned %>% filter(race == "Coloured" | race == "White")
#   # balanced data set with under-sampling
#   balanced.under <- ovun.sample(race~., data=africa,
#                                 p=0.5, seed=1,
#                                 method="under")$data
# 
#   # Balancing white and coloured
#   balanced.under1 <- ovun.sample(race~., data=coloured,
#                                  p=0.5, seed=1,
#                                  method="under")$data
# 
#   colour <- balanced.under1 %>% filter(race == "Coloured")
#   asia <- cleaned %>% filter(race == "Asian/Indian")
# 
#   bal_under <- bind_rows(balanced.under, colour, asia)
#   bal1 <- bal_under %>% tibble::as_tibble()
# # #str(cleaned)
# # #str(bal1)
# source("code/ML_Plot.R")
# balu <- ML_Plot(bal1) # running the machine learning algorithms on balanced (undersampled) data
# plot(balu) # I saved this plot as an image called bal1.png
```

```{r balance2 , include= FALSE, warning =  FALSE, fig.align = 'center', fig.cap = "ML on balanced data", fig.height = 3, fig.width = 6, dev = 'png'}

# # balaning the data using over and under sampling methods 
# list.files('code/', full.names = T, recursive = T) %>% as.list() %>% walk(~source(.))
# source("code/clean.R")
# cleaned <- clean(Adult)
# 
# ### This balancing section was also in a function but it kept breaking the document when I tried to knit so I unfortunately had to pull it out and put it here
# 
# library(ROSE)
# library(tidyverse)
# library(dplyr)
# 
#   africa <- cleaned %>% filter(race == "African" | race == "White") # First splitting the dataset
#   coloured <- cleaned %>% filter(race == "Coloured" | race == "African")
#   asian <- cleaned %>% filter(race == "Asian/Indian" | race == "African")
# 
# # balanced data set with both-sampling
#   balanced.both <- ovun.sample(race~., data=africa,
#                                p=0.5, seed=1,
#                                method="both")$data
# 
#   white <- balanced.both %>% filter(race == "White")
# 
#   # Balancing African and coloured
#   balanced.both1 <- ovun.sample(race~., data=coloured,
#                                 p=0.5, seed=1,
#                                 method="both")$data
# 
# 
# 
#   colour <- balanced.both1 %>% filter(race == "Coloured")
# 
#   # Balancing African and Asian
#   balanced.both2 <- ovun.sample(race~., data=asian,
#                                 p=0.5, seed=1,
#                                 method="both")$data
# 
#   asia <- balanced.both2 %>% filter(race == "Asian/Indian")
# 
#   bal_both <- bind_rows(balanced.both1, white, asia)
# 
#   bal2 <-  bal_both %>% tibble::as_tibble()
#   source("code/ML_Plot.R")
#   balb <- ML_Plot(bal1) # running the machine learning algorithms on balanced (both sampled) data
#   plot(balb) # I saved this plot as an image called bal2.png
```

\begin{figure}[htbp]
\centering
\includegraphics{images/unbal.png}
\caption{Machine Learning applied to unbalanced data \label{fig1}}
\end{figure}

\begin{figure}[htbp]
\centering
\includegraphics{images/bal1.png}
\caption{Machine Learning applied to balanced (undersampled) data \label{fig2}}
\end{figure}

\begin{figure}[htbp]
\centering
\includegraphics{images/bal2.png}
\caption{Machine Learning applied to balanced data \label{fig3}}
\end{figure}

Figure \ref{fig1} shows that the RF algorithm was (marginally) the most accurate in predicting a person's race for the unbalanced data. It had an accuracy measure of 62% (using a confidence level of 95%), where accuracy is the ratio of the number of correct predictions to the total number of input samples. The CART, SVM, and LDA models all performed similarly well, with accuracy statistics lying around 60%. The KNN model performed the worst according to the accuracy measure. However, accuracy isn't the only metric we should be concerned with; since the data is unbalanced, it may be a misleading statistic. For example, since a large part of the sample consists of the class African (let's say it's 80%), if a model predicts African 100% of the time, it would have an accuracy rating of 80%, which is not all that helpful.

A second metric we can look at is Cohen’s kappa (calculated based on the confusion matrix), which can help assess the performance of classification models. The kappa metric accounts for an imbalance in class distribution; this also makes it more difficult to interpret, but a higher kappa indicates a better performing model. According to this metric, all the models performed quite poorly. RF and CART have a measure of around 20% and the other models have a kappa round 10% and lower. 

Figure \ref{fig2} reports the results of the algorithms applied to the undersampled balanced data. We can see that the accuracy measures decreased for all the models. This could be attributed to a smaller sample size due to the balancing. SVM performed the best, followed by LDA, RF, CART and KNN. Balancing the data set impacted the kappa significantly, with all of the kappas increasing. The kappa for SVM improved from 10% in figure \ref{fig1} to 35% in \ref{fig2}. Similarly in \ref{fig3}, for the other balanced data, the kappa values for all the models improved from those in figure \ref{fig1}. The accuracy measures remain quite low for both balanced data sets, hovering below 60%. The poor predictive power of the model could be due to there being too few predictive variables, low information predictive variables or too small a sample size.

A more comprehensive way to gauge the performance of classification algorithms is to look at the confusion matrix. A confusion matrix summarizes the predictive results in a classification problem. Both correct and incorrect predictions are tabulated with their values and broken down by each class. The graph below (\ref{fig4}) displays the confusion matrix for the Random Forest results on the unbalanced data and the tables (\ref{fig5}) report the relevant statistics. 

\begin{figure}[htbp]
\centering
\includegraphics{images/cmrf.png}
\caption{Random Forest Confusion Matrix \label{fig4}}
\end{figure}

The y axis of figure \ref{fig4} indicates the actual values and the x axis indicates the predicted values. The first value indicated in the matrix show the number of predictions for that class. There are four outcomes possible for each prediction: true positive, true negative, false positive and false negative. The first outcome is where the model predicted the correct class. For example, a person who is African is predicted as African. The second outcome is that the model predicted that a person was not of a certain race, when they were not (for example, a person who is Coloured is not predicted as African). The third outcome is a false positive, where a person is predicted as a certain race when they are not. For example, a person who is White is predicted as Asian/Indian. And finally, the fourth outcome is where a person is predicted as not a certain race when they are. For example, a person who is Coloured is predicted as not Coloured. The confusion matrix and statistics measure these outcomes. For example, the bottom left box in the confusion matrix shows the true positive for the class African. There were 406 Africans who were correctly predicted as African. The bottom right box in the confusion matrix shows that there were 14 cases where a person was African but predicted as White.

The tables of \ref{fig5} detail further accuracy metrics. For example, the sensitivity value (also known as recall) for the class Coloured is 9%. This indicates how many cases we predicted correctly out of all the positive classes. The fact that this value is so low indicates RF was not successful at predicting when a person is Coloured. The model was even worse at predicting when a person is Asian/Indian with a specificity of 0% - we can see why this is: the model never once predicted that a person was Asian/Indian. Another metric we could look at is specificity, which determines the proportion of actual negatives that are correctly identified. For Asian/Indian this value is 100%, which means that for every case where a person was not Asian/Indian, the model did not predict them to be Asian/Indian. For African, the specificity was 15%, which means that for 15% of cases where a person was not African, the model did not predict them to be African.

Precision is the ratio of the total number of correctly classified positive classes to the total number of predicted positive classes. For African this was 63%, compared to 45% for Coloured and 51% for White. We would like precision to be high. To help compare models that have different precision and recall values, we can use the F-Score, which is the harmonic mean of recall and precision. Overall, the results of the RF are disappointing and show that the model does not have high predictive power. It does best at correctly predicting people who are African and very poorly at correctly predicting people who are Asian/Indian.

\begin{figure}[htbp]
\centering
\includegraphics{images/statsrf.png}
\caption{Random Forest Statistics \label{fig5}}
\end{figure}



```{r dpi=350, include = TRUE, warning =  FALSE, fig.align = 'center', fig.cap = "Confusion Matrix \\label{Figure3}", dev = 'png'}

# # I saved the output g1 as an image: cmsvm.png
# list.files('code/', full.names = T, recursive = T) %>% as.list() %>% walk(~source(.))
# cleaned <- clean(Adult)
# predicted <- ML_Predict(cleaned) # prediction results
# g1 <-draw_confusion_matrix(predicted[[5]]) # visualising the confusion matrix for rf
```

```{r Figure5, warning =  FALSE, fig.align = 'center', fig.height = 4, fig.width = 6, dev = 'png'}
# random forest stats
# I saved the output g2 as an image: statsrf.png
# list.files('code/', full.names = T, recursive = T) %>% as.list() %>% walk(~source(.))
# cleaned <- clean(Adult)
# predicted <- ML_Predict(cleaned) # prediction results
# g2 <- draw_confusion_stats(predicted[[5]]) # tabulating the statistics of random forest

```

# Conclusion \label{con}

The purpose of this essay was to explore machine learning techniques and SQL tools. To this end, I manipulated the NIDS data set using SQLite and learned how to integrate SQL and r. SQL has proven to be a powerful tool for working with big data sets and cleaning data. In section \ref{ML}, several machine learning techniques were applied to the NIDS data set, including linear and regularized regression as well as classification algorithms. The results were disappointing, with the machine learning techniques having low predictive power.


\newpage

# References {-}

<div id="refs"></div>


